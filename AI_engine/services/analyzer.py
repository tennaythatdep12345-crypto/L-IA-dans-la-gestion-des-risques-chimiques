# ai_engine/services/analyzer.py
"""
Service principal d'analyse des risques chimiques - LOGIQUE HSE RÃ‰VISÃ‰E
Projet IUT GÃ©nie Chimique - 1Ã¨re annÃ©e

NOUVELLE LOGIQUE (fÃ©vrier 2026):
1. DÃ©tection des rÃ©actions dangereuses CONNUES
2. Calcul sÃ©parÃ© des 3 scores (0-50 max)
3. AgrÃ©gation pondÃ©rÃ©e (20% inflam + 35% tox + 45% incomp)
4. Ajustements environnementaux MULTIPLICATIFS
5. Seuil minimum 50 pour rÃ©actions dangereuses
"""

from utils.csv_loader import (
    load_substances,
    load_incompatibilities,
    find_substance_by_name,
    find_substance_by_cas
)
from utils.processor import (
    standardize_chemical_name,
    clean_input,
    is_valid_chemical_name
)
from utils.environmental_factors import apply_environmental_adjustments, determine_risk_level
from config.dangerous_reactions import is_dangerous_reaction
from rules.inflammabilite import (
    evaluate_inflammability,
    get_safety_recommendations as get_inflammability_recommendations
)
from rules.toxicite import (
    evaluate_toxicity,
    get_safety_recommendations as get_toxicity_recommendations
)
from rules.incompatibilites import (
    evaluate_incompatibility,
    get_storage_recommendations,
    check_multiple_incompatibilities
)
from Scoring.risk_score import (
    calculate_global_risk_score,
    generate_risk_summary,
    get_recommendations_by_level
)


def analyze_risk(input_data):
    """
    Analyse les risques chimiques pour une ou plusieurs substances.
    
    Cette fonction principale coordonne l'ensemble du processus d'Ã©valuation:
    - Validation et normalisation des entrÃ©es
    - Recherche des substances dans la base de donnÃ©es
    - Ã‰valuation des risques individuels (inflammabilitÃ©, toxicitÃ©)
    - DÃ©tection des incompatibilitÃ©s entre substances
    - Calcul du score de risque global
    - GÃ©nÃ©ration de recommandations de sÃ©curitÃ©
    
    Args:
        input_data (dict): DonnÃ©es d'entrÃ©e contenant:
            - substances (list): Liste de noms de substances
            - quantites (dict, optional): QuantitÃ©s par substance (en mL ou g)
            - contexte_labo (dict, optional): Conditions de laboratoire
    
    Returns:
        dict: RÃ©sultat structurÃ© de l'analyse comprenant:
            - score_global (float): Score de risque global (0-100)
            - niveau_risque (str): Niveau qualitatif (FAIBLE, MOYEN, ELEVE)
            - details (dict): DÃ©tails par catÃ©gorie de risque
            - recommandations (list): Liste de recommandations de sÃ©curitÃ©
            - substances_analysees (list): DÃ©tails de chaque substance
            - erreurs (list): Liste des erreurs rencontrÃ©es
    
    Exemple:
        >>> data = {
        ...     "substances": ["Ethanol", "AcÃ©tone"],
        ...     "quantites": {"Ethanol": 500, "AcÃ©tone": 250}
        ... }
        >>> result = analyze_risk(data)
        >>> print(result['niveau_risque'])
    """
    # Initialisation du rÃ©sultat
    result = {
        'success': True,
        'score_global': 0,
        'niveau_risque': 'FAIBLE',
        'details': {
            'inflammabilite': {'score': 0, 'explication': ''},
            'toxicite': {'score': 0, 'explication': ''},
            'incompatibilites': []
        },
        'origines_risque': [],  # Pour afficher l'origine du risque
        'scenario_critique': '',  # Description du scÃ©nario critique
        'reactions_chimiques': [],  # RÃ©actions possibles
        'recommandations': [],
        'substances_analysees': [],
        'erreurs': [],
        'avertissements': []
    }
    
    # Ã‰TAPE 1: Validation des donnÃ©es d'entrÃ©e
    validation_errors = _validate_input(input_data)
    if validation_errors:
        result['success'] = False
        result['erreurs'] = validation_errors
        return result
    
    # Ã‰TAPE 2: Chargement des bases de donnÃ©es
    substances_db = load_substances()
    incompatibilities_db = load_incompatibilities()
    
    if not substances_db:
        result['success'] = False
        result['erreurs'].append("Impossible de charger la base de donnÃ©es des substances")
        return result
    
    # Ã‰TAPE 3: Extraction et normalisation des substances demandÃ©es
    substance_names = input_data.get('substances', [])
    quantities = input_data.get('quantites', {})
    context = input_data.get('contexte_labo', {})
    
    print(f"[ANALYZER DEBUG] Input data received: {input_data}")
    print(f"[ANALYZER DEBUG] Context extracted: {context}")
    
    # Recherche et validation de chaque substance
    found_substances = []
    for name in substance_names:
        substance_data = _find_and_validate_substance(name, substances_db, result)
        if substance_data:
            substance_data['quantite'] = quantities.get(name, 0)
            found_substances.append(substance_data)
    
    if not found_substances:
        result['success'] = False
        result['erreurs'].append("Aucune substance valide n'a pu Ãªtre identifiÃ©e")
        return result
    
    # Ã‰TAPE 4: Ã‰valuation des risques individuels pour chaque substance
    all_inflammability_scores = []
    all_toxicity_scores = []
    
    for substance in found_substances:
        # Ã‰valuation de l'inflammabilitÃ©
        inflam_result = evaluate_inflammability(substance)
        
        # Ã‰valuation de la toxicitÃ©
        tox_result = evaluate_toxicity(substance)
        
        # Stockage des rÃ©sultats
        all_inflammability_scores.append(inflam_result['score'])
        all_toxicity_scores.append(tox_result['score'])
        
        # Ajout des dÃ©tails pour cette substance
        result['substances_analysees'].append({
            'nom': substance.get('nom'),
            'cas': substance.get('cas', 'N/A'),
            'quantite': substance.get('quantite', 0),
            'inflammabilite': {
                'score': inflam_result['score'],
                'niveau': inflam_result['niveau'],
                'explication': inflam_result['explication']
            },
            'toxicite': {
                'score': tox_result['score'],
                'niveau': tox_result['niveau'],
                'explication': tox_result['explication']
            }
        })
    
    # Ã‰TAPE 5: Calcul des scores moyens pour inflammabilitÃ© et toxicitÃ©
    avg_inflammability = sum(all_inflammability_scores) / len(all_inflammability_scores) if all_inflammability_scores else 0
    avg_toxicity = sum(all_toxicity_scores) / len(all_toxicity_scores) if all_toxicity_scores else 0
    
    # Prise du score maximum plutÃ´t que moyenne pour Ãªtre plus conservateur
    max_inflammability = max(all_inflammability_scores) if all_inflammability_scores else 0
    max_toxicity = max(all_toxicity_scores) if all_toxicity_scores else 0
    
    # Ã‰TAPE 6: Ã‰valuation des incompatibilitÃ©s + DÃ‰TECTION RÃ‰ACTIONS DANGEREUSES
    incompatibility_score = 0
    incompatibility_details = []
    is_dangerous_reaction_detected = False
    dangerous_reaction_info = None
    
    print(f"[ANALYZER DEBUG] Found substances count: {len(found_substances)}")
    print(f"[ANALYZER DEBUG] Found substances: {[s.get('nom') for s in found_substances]}")
    print(f"[ANALYZER DEBUG] Incompatibilities DB count: {len(incompatibilities_db)}")
    
    if len(found_substances) > 1:
        # DÃ©tection de toutes les incompatibilitÃ©s
        detected_incomp = check_multiple_incompatibilities(found_substances, incompatibilities_db)
        
        print(f"[ANALYZER DEBUG] Detected incompatibilities: {len(detected_incomp)}")
        for inc in detected_incomp:
            print(f"[ANALYZER DEBUG]   - {inc['substance1']} + {inc['substance2']}: score={inc['score']}")
        
        if detected_incomp:
            # Prise du score d'incompatibilitÃ© le plus Ã©levÃ©
            incompatibility_score = max([inc['score'] for inc in detected_incomp])
            
            # Stockage des dÃ©tails et dÃ©tection de rÃ©actions dangereuses
            for inc in detected_incomp:
                # VÃ‰RIFIER SI C'EST UNE RÃ‰ACTION DANGEREUSE CONNUE
                is_dangerous, danger_info = is_dangerous_reaction(inc['substance1'], inc['substance2'])
                
                if is_dangerous:
                    is_dangerous_reaction_detected = True
                    dangerous_reaction_info = {
                        'substances': [inc['substance1'], inc['substance2']],
                        'produit': danger_info.get('produit'),
                        'formule': danger_info.get('formule'),
                        'toxicite': danger_info.get('toxicite'),
                        'score_minimum': danger_info.get('min_score', 50),
                        'recommandations': danger_info.get('recommandations', [])
                    }
                    print(f"[ANALYZER DEBUG] REACTION DANGEREUSE DETECTEE: {inc['substance1']} + {inc['substance2']} -> {danger_info.get('produit')}")
                
                incompatibility_details.append({
                    'substances': [inc['substance1'], inc['substance2']],
                    'score': inc['score'],
                    'niveau': inc['niveau'],
                    'explication': inc['explication'],
                    'is_dangerous': is_dangerous,
                    'dangerous_info': danger_info if is_dangerous else None
                })
                
                # AJOUTER Ã€ reactions_chimiques pour affichage frontend
                if inc.get('equation_reaction') or inc.get('produit_reaction'):
                    result['reactions_chimiques'].append({
                        'substances': f"{inc['substance1']} + {inc['substance2']}",
                        'product': inc.get('produit_reaction', ''),
                        'formula': inc.get('formule_produit', ''),
                        'equation': inc.get('equation_reaction', ''),
                        'justification': inc.get('justification', ''),
                        'risk_level': inc.get('niveau', 'MOYEN'),
                        'type_reaction': inc.get('type_reaction', '')
                    })
                
                # Ajout des recommandations de stockage
                sub1_data = next((s for s in found_substances if s.get('nom') == inc['substance1']), None)
                sub2_data = next((s for s in found_substances if s.get('nom') == inc['substance2']), None)
                
                if sub1_data and sub2_data:
                    storage_recs = get_storage_recommendations(sub1_data, sub2_data, incompatibilities_db)
                    result['recommandations'].extend(storage_recs)
    
    # Ã‰TAPE 7: AgrÃ©gation des scores avec NOUVELLE LOGIQUE HSE
    individual_scores = {
        'inflammabilite': max_inflammability,
        'toxicite': max_toxicity,
        'incompatibilites': incompatibility_score
    }
    
    # Calcul du score global HSE: 20% inflamm + 35% tox + 45% incomp
    global_result = calculate_global_risk_score(
        individual_scores,
        is_dangerous_reaction_detected=is_dangerous_reaction_detected
    )
    
    base_score = global_result['score_global']  # Score 0-50 AVANT facteurs environnementaux
    print(f"[ANALYZER DEBUG] Base score (0-50): {base_score}")
    print(f"[ANALYZER DEBUG] Dangerous reaction detected: {is_dangerous_reaction_detected}")
    
    # Ã‰TAPE 7.5: Ajustement du score en fonction des conditions environnementales (MULTIPLICATIF)
    temperature = context.get('temperature_c')
    humidity = context.get('humidite_percent')
    ventilation = context.get('ventilation')  # Oui/Non
    final_score = base_score
    environmental_factors = {}
    
    print(f"[ANALYZER DEBUG] Temperature: {temperature}, Humidity: {humidity}, Ventilation: {ventilation}")
    
    if temperature is not None and humidity is not None:
        print(f"[ANALYZER DEBUG] Calculating environmental adjustments (MULTIPLICATIVE)...")
        
        # Appliquer les ajustements MULTIPLICATIFS
        final_score = apply_environmental_adjustments(
            base_score,
            temperature_c=temperature,
            humidity_percent=humidity,
            ventilation=ventilation,
            is_dangerous_reaction=is_dangerous_reaction_detected
        )
        
        print(f"[ANALYZER DEBUG] Original score: {base_score}, Adjusted: {final_score}")
        
        environmental_factors = {
            'temperature_c': temperature,
            'humidity_percent': humidity,
            'ventilation': ventilation,
            'base_score': base_score,
            'final_score': final_score
        }
        
        result['environnemental_factors'] = environmental_factors
    else:
        print(f"[ANALYZER DEBUG] Environmental factors NOT calculated - temperature or humidity is None")
    # Ã‰TAPE 8: Construction du rÃ©sultat final avec NOUVEAU SCORING
    result['score_global'] = final_score
    
    # DÃ©terminer le niveau de risque basÃ© sur le score final (utiliser fonction du module)
    from Scoring.risk_score import get_risk_level_only
    risk_level = get_risk_level_only(final_score)
    result['niveau_risque'] = risk_level
    
    print(f"[ANALYZER DEBUG] Final result: score={final_score}, level={risk_level}")
    
    result['details']['inflammabilite'] = {
        'score': max_inflammability,
        'score_moyen': round(avg_inflammability, 1),
        'explication': f"Score maximum d'inflammabilitÃ© parmi les substances: {max_inflammability}"
    }
    
    result['details']['toxicite'] = {
        'score': max_toxicity,
        'score_moyen': round(avg_toxicity, 1),
        'explication': f"Score maximum de toxicitÃ© parmi les substances: {max_toxicity}"
    }
    
    result['details']['incompatibilites'] = incompatibility_details
    
    # Ajouter les scores pondÃ©rÃ©s (dÃ©composition du score global)
    result['details']['scores_ponderes'] = global_result['scores_ponderes']
    result['details']['base_score'] = base_score
    result['details']['final_score'] = final_score
    result['details']['explication_globale'] = global_result['explication']
    
    if is_dangerous_reaction_detected and dangerous_reaction_info:
        result['details']['dangerous_reaction'] = dangerous_reaction_info
    
    # CrÃ©er un mapping pour 'scores_details' (utilisÃ© par le frontend)
    result['scores_details'] = {
        'inflammabilite': max_inflammability,
        'toxicite': max_toxicity,
        'incompatibilites': incompatibility_score
    }
    
    # Ã‰TAPE 8.5: CrÃ©er l'origine du risque basÃ©e sur les scores
    origines = []
    if max_inflammability >= 40:
        origines.append({'icon': 'ğŸ”¥', 'text': 'Inflam...'})  # InflammabilitÃ©
    if max_toxicity >= 40:
        origines.append({'icon': 'â˜ ï¸', 'text': 'Tox...'})  # ToxicitÃ©
    if incompatibility_score >= 40:
        origines.append({'icon': 'âš¡', 'text': 'Inco...'})  # IncompatibilitÃ©
    if max_inflammability >= 50:
        origines.append({'icon': 'ğŸ”§', 'text': 'RÃ©ac...'})  # RÃ©activitÃ©
    
    result['origines_risque'] = origines[:4]  # Limiter Ã  4
    
    # CrÃ©er un scÃ©nario critique basÃ© sur les risques
    if is_dangerous_reaction_detected and dangerous_reaction_info:
        result['scenario_critique'] = f"ğŸš¨ RÃ‰ACTION DANGEREUSE CONNUE: {dangerous_reaction_info['produit']} ({dangerous_reaction_info['formule']}) - {dangerous_reaction_info['toxicite']}"
    elif max_inflammability >= 70 and max_toxicity >= 70:
        result['scenario_critique'] = f"MÃ©lange accidentel de substances hautement inflammables et toxiques - formation possible de composÃ©s dangereux, gaz toxiques, potentiel d'explosion"
    elif max_inflammability >= 70:
        result['scenario_critique'] = f"RÃ©action de combustion rapide du mÃ©lange - potentiel d'explosion, projection de matiÃ¨res chaudes"
    elif max_toxicity >= 70:
        result['scenario_critique'] = f"DÃ©gagement de gaz toxiques en forte concentration - risque d'intoxication aigÃ¼e"
    elif incompatibility_score >= 70:
        result['scenario_critique'] = f"RÃ©action vigoureuse entre les substances - dÃ©gagement de chaleur, projection de produits"
    else:
        result['scenario_critique'] = f"Exposition Ã  des substances prÃ©sentant un risque modÃ©rÃ© - respect des mesures de sÃ©curitÃ© requis"
    
    # Ã‰TAPE 9: GÃ©nÃ©ration des recommandations gÃ©nÃ©rales
    general_recs = get_recommendations_by_level(final_score)  # Utiliser le score final pour les recommandations
    result['recommandations'].extend(general_recs)
    
    # Ajout de recommandations spÃ©cifiques selon les risques identifiÃ©s
    if max_inflammability >= 60:
        result['recommandations'].append("âš ï¸ Risque d'inflammabilitÃ© Ã©levÃ© dÃ©tectÃ© : Ã©loigner toute source d'ignition")
    
    if max_toxicity >= 70:
        result['recommandations'].append("â˜ ï¸ Risque toxicologique Ã©levÃ© dÃ©tectÃ© : manipulation sous hotte obligatoire")
    
    if incompatibility_score >= 60:
        result['recommandations'].append("ğŸ”´ IncompatibilitÃ©s sÃ©vÃ¨res dÃ©tectÃ©es : ne jamais mÃ©langer ces substances")
    
    # Ajouter les recommandations spÃ©cifiques de rÃ©action dangereuse
    if is_dangerous_reaction_detected and dangerous_reaction_info:
        for rec in dangerous_reaction_info.get('recommandations', []):
            result['recommandations'].append(f"ğŸš¨ {rec}")
    
    # Prise en compte du contexte de laboratoire
    if context:
        context_warnings = _evaluate_context(context, individual_scores, environmental_factors)
        result['avertissements'].extend(context_warnings)
    
    # Suppression des doublons dans les recommandations
    result['recommandations'] = list(dict.fromkeys(result['recommandations']))
    
    return result


def _validate_input(input_data):
    """
    Valide les donnÃ©es d'entrÃ©e.
    
    Args:
        input_data (dict): DonnÃ©es Ã  valider
    
    Returns:
        list: Liste des erreurs de validation (vide si tout est valide)
    """
    errors = []
    
    if not isinstance(input_data, dict):
        errors.append("Les donnÃ©es d'entrÃ©e doivent Ãªtre un dictionnaire")
        return errors
    
    if 'substances' not in input_data:
        errors.append("Le champ 'substances' est obligatoire")
        return errors
    
    substances = input_data.get('substances', [])
    
    if not isinstance(substances, list):
        errors.append("Le champ 'substances' doit Ãªtre une liste")
        return errors
    
    if len(substances) == 0:
        errors.append("La liste de substances ne peut pas Ãªtre vide")
        return errors
    
    if len(substances) > 10:
        errors.append("Trop de substances (maximum 10 par analyse)")
    
    # Validation des noms de substances
    for name in substances:
        if not is_valid_chemical_name(name):
            errors.append(f"Nom de substance invalide: '{name}'")
    
    return errors


def _find_and_validate_substance(name, substances_db, result):
    """
    Recherche et valide une substance dans la base de donnÃ©es.
    
    Args:
        name (str): Nom de la substance Ã  rechercher
        substances_db (dict): Base de donnÃ©es des substances
        result (dict): Dictionnaire de rÃ©sultat pour stocker les erreurs/avertissements
    
    Returns:
        dict or None: DonnÃ©es de la substance si trouvÃ©e, None sinon
    """
    # Normalisation du nom
    cleaned_name = clean_input(name)
    
    # Recherche par nom
    substance = find_substance_by_name(cleaned_name, substances_db)
    
    if substance:
        return substance
    
    # Si pas trouvÃ©e, essayer de rechercher par CAS si le format correspond
    if '-' in cleaned_name:
        substance = find_substance_by_cas(cleaned_name, substances_db)
        if substance:
            return substance
    
    # Substance non trouvÃ©e
    result['avertissements'].append(
        f"Substance '{name}' non trouvÃ©e dans la base de donnÃ©es. "
        f"Ã‰valuation basÃ©e sur des valeurs par dÃ©faut."
    )
    
    # CrÃ©ation d'une substance par dÃ©faut avec donnÃ©es minimales
    return {
        'nom': cleaned_name,
        'cas': None,
        'point_eclair': None,
        'toxicite': 'NOCIF',
        'categorie': 'non_classee'
    }


def _evaluate_context(context, individual_scores, environmental_factors=None):
    """
    Ã‰value les conditions de laboratoire et gÃ©nÃ¨re des avertissements si nÃ©cessaire.
    
    Args:
        context (dict): Conditions de laboratoire
        individual_scores (dict): Scores individuels des risques
        environmental_factors (dict): RÃ©sultats du calcul des facteurs environnementaux
    
    Returns:
        list: Liste d'avertissements contextuels
    """
    warnings = []
    
    # VÃ©rification de la ventilation
    ventilation = context.get('ventilation', True)
    if not ventilation and individual_scores['toxicite'] >= 40:
        warnings.append("âš ï¸ Absence de ventilation avec substances toxiques : risque accru d'intoxication")
    
    if not ventilation and individual_scores['inflammabilite'] >= 50:
        warnings.append("âš ï¸ Absence de ventilation avec substances inflammables : risque d'accumulation de vapeurs")
    
    # Utiliser les facteurs environnementaux calculÃ©s si disponibles
    if environmental_factors:
        # Avertissements gÃ©nÃ©rÃ©s par le calculateur de taux de rÃ©action
        for warning in environmental_factors.get('warnings', []):
            if 'âœ…' not in warning:  # Ne pas inclure les avertissements positifs
                warnings.append(warning)
        
        # Recommandations de sÃ©curitÃ© basÃ©es sur la tempÃ©rature
        temp = environmental_factors.get('temperature')
        if temp and temp > 50:
            temp_rec = f"ğŸŒ¡ï¸ TempÃ©rature Ã  {temp}Â°C : multiplicateur de rÃ©action = {environmental_factors.get('reaction_rate_multiplier', 1.0)}x"
            warnings.append(temp_rec)
    else:
        # Fallback Ã  la vÃ©rification simple de la tempÃ©rature (ancien code)
        temperature = context.get('temperature_c')
        if temperature and temperature > 25 and individual_scores['inflammabilite'] >= 60:
            warnings.append(f"âš ï¸ TempÃ©rature Ã©levÃ©e ({temperature}Â°C) : augmente le risque d'inflammation")
    
    # VÃ©rification de l'humiditÃ© (si fournie)
    humidite = context.get('humidite_percent')
    if humidite and humidite < 30:
        warnings.append("âš ï¸ Faible humiditÃ© : risque accru d'Ã©lectricitÃ© statique")
    
    return warnings


def analyze_single_substance(substance_name):
    """
    Analyse simplifiÃ©e pour une seule substance.
    
    Args:
        substance_name (str): Nom de la substance Ã  analyser
    
    Returns:
        dict: RÃ©sultat de l'analyse
    """
    input_data = {
        'substances': [substance_name]
    }
    
    return analyze_risk(input_data)


def analyze_substance_pair(substance1, substance2):
    """
    Analyse ciblÃ©e sur les incompatibilitÃ©s entre deux substances.
    
    Args:
        substance1 (str): Nom de la premiÃ¨re substance
        substance2 (str): Nom de la deuxiÃ¨me substance
    
    Returns:
        dict: RÃ©sultat de l'analyse focalisÃ©e sur les incompatibilitÃ©s
    """
    input_data = {
        'substances': [substance1, substance2]
    }
    
    return analyze_risk(input_data)


def get_substance_info(substance_name):
    """
    RÃ©cupÃ¨re les informations dÃ©taillÃ©es d'une substance.
    
    Args:
        substance_name (str): Nom de la substance
    
    Returns:
        dict or None: Informations de la substance
    """
    substances_db = load_substances()
    return find_substance_by_name(substance_name, substances_db)


def list_available_substances():
    """
    Liste toutes les substances disponibles dans la base de donnÃ©es.
    
    Returns:
        list: Liste des noms de substances disponibles
    """
    substances_db = load_substances()
    return [sub['nom'] for sub in substances_db.values()]